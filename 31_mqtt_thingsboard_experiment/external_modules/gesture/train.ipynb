{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 01 数据集导入，创建训练集和测试集。通过查看注释，理解整个流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1232, 60) (308, 60) (1232,) (308,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "# 训练集、测试集划分\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 数据集相对路径\n",
    "DATA_PATH = \"../../../10_tingml_datasets/\"\n",
    "# LABELS 的内容尽量与前面store_data.py保持一致\n",
    "LABELS = [\"Stationary\", \"Tilted\", \"Rotating\", \"Moving\"]\n",
    "# 代表一个样本内容，如连续10次传感器读到的6轴数据作为一个样本\n",
    "SAMPLES_PER_GESTURE = 10\n",
    "def load_one_label_data(label):\n",
    "    path = DATA_PATH + label + \"*.npy\"\n",
    "    files = glob.glob(path)\n",
    "    datas = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            data = np.load(file)\n",
    "            # 切除多余数据，如数据当中有61份，但每个样本只需要10份，那么最后一份需要丢弃。\n",
    "            num_slice = len(data) // SAMPLES_PER_GESTURE\n",
    "            datas.append(data[: num_slice * SAMPLES_PER_GESTURE, :])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    datas = np.concatenate(datas, axis=0)\n",
    "    # 由于本案例给的是全连接层，输入为1维数据。(其余如conv需要自行根据模型输入修改尺寸，如二维)\n",
    "    # MLP\n",
    "    # datas = np.reshape(datas,(-1, 6 * SAMPLES_PER_GESTURE,),)  # Modified here\n",
    "    # CNN 1\n",
    "    datas = np.reshape(datas,(-1, 6 * SAMPLES_PER_GESTURE, 1),)  # Modified here\n",
    "    idx = LABELS.index(label)\n",
    "    labels = np.ones(datas.shape[0]) * idx\n",
    "    return datas, labels\n",
    "all_datas = []\n",
    "all_labels = []\n",
    "# 导入每个label对应的数据\n",
    "for label in LABELS:\n",
    "    datas, labels = load_one_label_data(label)\n",
    "    all_datas.append(datas)\n",
    "    all_labels.append(labels)\n",
    "dataX = np.concatenate(all_datas, axis=0)\n",
    "dataY = np.concatenate(all_labels, axis=0)\n",
    "# 输入和样本到此创建完毕\n",
    "\n",
    "# 训练集、测试集划分\n",
    "# test_size 表示数据集里面有20%将划分给测试集\n",
    "# stratify=dataY指定按label进行划分, 确保数据集划分公平\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(\n",
    "    dataX, dataY, test_size=0.2, stratify=dataY\n",
    ")\n",
    "print(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02 模型创建\n",
    "下面将创建很简单的多层感知机模型，后续可自行定义模型结构。需要根据自身需求，自行上网查询其他模型，如CNN，切记模型不要太大，嵌入式设备大致提供32K空间供运行模型。\n",
    "模型需要注意输入尺寸，如CNN往往多维数据，如**Conv1d 输入二维，可将输入改为(6 * SAMPLES_PER_GESTURE,1)或者(SAMPLES_PER_GESTURE, 6), 上面数据集对应尺寸也需要修改**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                3904      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,164\n",
      "Trainable params: 4,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 0 = INFO, 1 = WARNING, 2 = ERROR, 3 = FATAL\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# 设置环境变量，控制日志级别\n",
    "def mlp():\n",
    "    # 一个用于线性堆叠多个网络层的模型。\n",
    "    # Sequential模型是最简单的神经网络模型，它按照层的顺序依次堆叠，每一层的输出会成为下一层的输入。\n",
    "    model = keras.Sequential()\n",
    "    # 第一层, 添加全连接层，输出尺寸为64，激活函数采用\"relu\"\n",
    "    # 第一层需要制定输入大小，这里和数据集对应input_shape=(6 * SAMPLES_PER_GESTURE,)\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\", input_shape=(6 * SAMPLES_PER_GESTURE,)))\n",
    "    # 添加池化层，防止模型过拟合，每次自动忘记20%的参数\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    # 最后一层，全连接层，输出尺寸对应labels数量，激活函数采用\"softmax\"\n",
    "    # softmaxs输出的结果代表每个label的概率，如第0个代表label 0的概率\n",
    "    model.add(keras.layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "    return model\n",
    "def cnn():\n",
    "    # 一个用于线性堆叠多个网络层的模型。\n",
    "    # Sequential模型是最简单的神经网络模型，它按照层的顺序依次堆叠，每一层的输出会成为下一层的输入。\n",
    "    model = keras.Sequential()\n",
    "    # 注意CNN与MLP的输入shape\n",
    "    # 16个输出通道，3为卷积核大小\n",
    "    model.add(\n",
    "        keras.layers.Conv1D(\n",
    "            8,3,padding=\"same\",activation=\"relu\",input_shape=(6 * SAMPLES_PER_GESTURE, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.Conv1D(8, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnn()\n",
    "# 打印模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 03 模型训练及测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 3.7269 - sparse_categorical_accuracy: 0.5950\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.88312, saving model to best_model.h5\n",
      "154/154 [==============================] - 4s 8ms/step - loss: 3.7269 - sparse_categorical_accuracy: 0.5950 - val_loss: 0.3131 - val_sparse_categorical_accuracy: 0.8831\n",
      "Epoch 2/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6096 - sparse_categorical_accuracy: 0.8359\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.88312 to 0.94805, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.6026 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.2033 - val_sparse_categorical_accuracy: 0.9481\n",
      "Epoch 3/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3035 - sparse_categorical_accuracy: 0.9034\n",
      "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.94805\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.3035 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.1438 - val_sparse_categorical_accuracy: 0.9481\n",
      "Epoch 4/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.2817 - sparse_categorical_accuracy: 0.9196\n",
      "Epoch 4: val_sparse_categorical_accuracy improved from 0.94805 to 0.96104, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2817 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.1212 - val_sparse_categorical_accuracy: 0.9610\n",
      "Epoch 5/50\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.2503 - sparse_categorical_accuracy: 0.9413\n",
      "Epoch 5: val_sparse_categorical_accuracy improved from 0.96104 to 0.97727, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2454 - sparse_categorical_accuracy: 0.9424 - val_loss: 0.1023 - val_sparse_categorical_accuracy: 0.9773\n",
      "Epoch 6/50\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1523 - sparse_categorical_accuracy: 0.9578\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.97727 to 0.98052, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1497 - sparse_categorical_accuracy: 0.9570 - val_loss: 0.0871 - val_sparse_categorical_accuracy: 0.9805\n",
      "Epoch 7/50\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1359 - sparse_categorical_accuracy: 0.9675\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.98052 to 0.99351, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.1371 - sparse_categorical_accuracy: 0.9659 - val_loss: 0.0890 - val_sparse_categorical_accuracy: 0.9935\n",
      "Epoch 8/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1483 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.99351\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.1497 - sparse_categorical_accuracy: 0.9651 - val_loss: 0.0694 - val_sparse_categorical_accuracy: 0.9773\n",
      "Epoch 9/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1133 - sparse_categorical_accuracy: 0.9662\n",
      "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.99351\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.0635 - val_sparse_categorical_accuracy: 0.9935\n",
      "Epoch 10/50\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.0966 - sparse_categorical_accuracy: 0.9725\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.99351\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0982 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.0556 - val_sparse_categorical_accuracy: 0.9935\n",
      "Epoch 11/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0946 - sparse_categorical_accuracy: 0.9755\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.99351\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0942 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.0445 - val_sparse_categorical_accuracy: 0.9903\n",
      "Epoch 12/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0838 - sparse_categorical_accuracy: 0.9837\n",
      "Epoch 12: val_sparse_categorical_accuracy improved from 0.99351 to 1.00000, saving model to best_model.h5\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0838 - sparse_categorical_accuracy: 0.9838 - val_loss: 0.0529 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1075 - sparse_categorical_accuracy: 0.9796\n",
      "Epoch 13: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1075 - sparse_categorical_accuracy: 0.9797 - val_loss: 0.0384 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 14/50\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.0786 - sparse_categorical_accuracy: 0.9795\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0308 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 15/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.0828 - sparse_categorical_accuracy: 0.9821\n",
      "Epoch 15: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9821 - val_loss: 0.0308 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.0434 - sparse_categorical_accuracy: 0.9925\n",
      "Epoch 16: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0235 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.0586 - sparse_categorical_accuracy: 0.9891\n",
      "Epoch 17: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0282 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.0511 - sparse_categorical_accuracy: 0.9910\n",
      "Epoch 18: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0235 - val_sparse_categorical_accuracy: 0.9935\n",
      "Epoch 19/50\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.0448 - sparse_categorical_accuracy: 0.9853\n",
      "Epoch 19: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.0201 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.0405 - sparse_categorical_accuracy: 0.9925\n",
      "Epoch 20: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0168 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.0344 - sparse_categorical_accuracy: 0.9925\n",
      "Epoch 21: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0166 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.0328 - sparse_categorical_accuracy: 0.9917\n",
      "Epoch 22: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0268 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0266 - sparse_categorical_accuracy: 0.9959\n",
      "Epoch 23: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0148 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0230 - sparse_categorical_accuracy: 0.9918\n",
      "Epoch 24: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0102 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.0241 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 25: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0153 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 26/50\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.0225 - sparse_categorical_accuracy: 0.9950\n",
      "Epoch 26: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0095 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 27: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0118 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 28/50\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.0239 - sparse_categorical_accuracy: 0.9941\n",
      "Epoch 28: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.0204 - sparse_categorical_accuracy: 0.9968\n",
      "Epoch 29: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.0086 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 0.9974\n",
      "Epoch 30: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0079 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 31/50\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.0319 - sparse_categorical_accuracy: 0.9930\n",
      "Epoch 31: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9935 - val_loss: 0.0163 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 32/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0344 - sparse_categorical_accuracy: 0.9949\n",
      "Epoch 32: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 8ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9935 - val_loss: 0.0182 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 33/50\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.0242 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 33: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0134 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 34/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 34: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0257 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 35/50\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.0166 - sparse_categorical_accuracy: 0.9950\n",
      "Epoch 35: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 8ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0159 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 36/50\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.0135 - sparse_categorical_accuracy: 0.9976\n",
      "Epoch 36: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0186 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 37/50\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.0282 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 37: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0090 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 38/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0120 - sparse_categorical_accuracy: 0.9975\n",
      "Epoch 38: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0086 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 39/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.0119 - sparse_categorical_accuracy: 0.9975\n",
      "Epoch 39: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0066 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.0098 - sparse_categorical_accuracy: 0.9975\n",
      "Epoch 40: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0093 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 41/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0215 - sparse_categorical_accuracy: 0.9983\n",
      "Epoch 41: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0117 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 42/50\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.0110 - sparse_categorical_accuracy: 0.9959\n",
      "Epoch 42: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0086 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 43/50\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.0069 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 43: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0080 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 44/50\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.0116 - sparse_categorical_accuracy: 0.9974\n",
      "Epoch 44: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.0080 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 45/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.0078 - sparse_categorical_accuracy: 0.9975\n",
      "Epoch 45: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0046 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.0360 - sparse_categorical_accuracy: 0.9949\n",
      "Epoch 46: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0350 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0154 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 47/50\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.0127 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 47: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0045 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 48/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.0059 - sparse_categorical_accuracy: 0.9992\n",
      "Epoch 48: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0119 - val_sparse_categorical_accuracy: 0.9968\n",
      "Epoch 49/50\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.0071 - sparse_categorical_accuracy: 0.9992\n",
      "Epoch 49: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0031 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 0.9992\n",
      "Epoch 50: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "154/154 [==============================] - 1s 7ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0034 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# 加载模型\n",
    "from tensorflow.keras.models import load_model\n",
    "# 测试模型性能\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 模型训练优化器，学习率为0.001\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "# 制定模型优化器，和损失函数、评价指标\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "# 制定保存模型的路径\n",
    "filepath = \"best_model.h5\"\n",
    "# 训练时，保存最好模型\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor=\"val_sparse_categorical_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "# 模型训练, 训练集，batch_size为批大小，可提高训练速度\n",
    "# validation_data指明验证集，epochs表示训练迭代轮数\n",
    "# verbose=1表示打印训练日志\n",
    "# callbacks调用上述保存模型的方法\n",
    "history = model.fit(\n",
    "    xTrain,\n",
    "    yTrain,\n",
    "    batch_size=8,\n",
    "    validation_data=(xTest, yTest),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "# 至此模型训练完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n",
      "[[68  0  0  0]\n",
      " [ 0 80  0  0]\n",
      " [ 0  0 88  0]\n",
      " [ 0  0  0 72]]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = load_model(filepath)\n",
    "# 模型推理，预测\n",
    "predictions = model.predict(xTest)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "# 查看混淆矩阵，效果越好，预测则集中在对角线。\n",
    "cm = confusion_matrix(yTest, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于运动状态简单，上述最简单的模型可能也会获得不错的性能，当运动状态变得复杂，上述MLP模型性能将很难满足需求。\n",
    "#### 04 生成最终部署的模型\n",
    "由于RIOT系统使用的时tflite-micro库且资源有限，将模型量化，并保存成tflite-micro可识别的格式，注意\n",
    "`data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, ))`后的尺寸维度和大小需要和前面大致对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa2yyizl2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa2yyizl2/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy16m180h/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy16m180h/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "/home/user/.local/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic model is 18344 bytes\n",
      "Quantized model is 6688 bytes\n",
      "Difference is 11656 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "# 加载模型\n",
    "model = load_model(filepath)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# 保存初始版本，后续对比用\n",
    "open(\"model_basic.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# 量化模型, 定义输入格式与大小，只需要修改(-1, 6 * SAMPLES_PER_GESTURE,)与上面对应即可，其余不用变\n",
    "data_test = xTest.astype(\"float32\")\n",
    "# np.reshape 和一开始数据集导入对应\n",
    "# MLP\n",
    "# data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, ))\n",
    "# CNN 1\n",
    "data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, 1))\n",
    "data_ds = tf.data.Dataset.from_tensor_slices((data_test)).batch(1)\n",
    "# Rest of your code...\n",
    "def representative_data_gen():\n",
    "    for input_value in data_ds.take(100):\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# 量化前后对比\n",
    "basic_model_size = os.path.getsize(\"model_basic.tflite\")\n",
    "print(\"Basic model is %d bytes\" % basic_model_size)\n",
    "quantized_model_size = os.path.getsize(\"model.tflite\")\n",
    "print(\"Quantized model is %d bytes\" % quantized_model_size)\n",
    "difference = basic_model_size - quantized_model_size\n",
    "print(\"Difference is %d bytes\" % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面，通过量化，帮助我们模型节省了2440Bytes大小。需要验证量化后的模型输入格式和尺寸是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 0.0 - Prediction:\n",
      "[[0.87890625 0.01171875 0.10546875 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 2.0 - Prediction:\n",
      "[[0.         0.         0.99609375 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 0.0 - Prediction:\n",
      "[[0.87890625 0.01171875 0.10546875 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 1.0 - Prediction:\n",
      "[[0.         0.99609375 0.         0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 0.0 - Prediction:\n",
      "[[0.87890625 0.01171875 0.10546875 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 2.0 - Prediction:\n",
      "[[0.09765625 0.09765625 0.8046875  0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 1.0 - Prediction:\n",
      "[[0.         0.99609375 0.         0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 0.0 - Prediction:\n",
      "[[0.8046875  0.09765625 0.09765625 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 2.0 - Prediction:\n",
      "[[0.015625 0.015625 0.96875  0.      ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 0.0 - Prediction:\n",
      "[[0.87890625 0.01171875 0.10546875 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 1.0 - Prediction:\n",
      "[[0.         0.99609375 0.         0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 2.0 - Prediction:\n",
      "[[0.         0.         0.99609375 0.        ]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 3.0 - Prediction:\n",
      "[[0.         0.         0.         0.99609375]]\n",
      "\n",
      "(1, 60)\n",
      "Digit: 2.0 - Prediction:\n",
      "[[0.87890625 0.01171875 0.10546875 0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Now let's verify the model on a few input digits\n",
    "# Instantiate an interpreter for the model\n",
    "model_quantized_reloaded = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "# Allocate memory for each model\n",
    "model_quantized_reloaded.allocate_tensors()\n",
    "\n",
    "# Get the input and output tensors so we can feed in values and get the results\n",
    "model_quantized_input = model_quantized_reloaded.get_input_details()[0][\"index\"]\n",
    "model_quantized_output = model_quantized_reloaded.get_output_details()[0][\"index\"]\n",
    "# Create arrays to store the results\n",
    "model_quantized_predictions = np.empty(xTest.size)\n",
    "for i in range(20):\n",
    "    # Reshape the data and ensure the type is float32\n",
    "    # test_data = np.reshape(\n",
    "    #     xTest[i],\n",
    "    #     (\n",
    "    #         1,\n",
    "    #         6 * SAMPLES_PER_GESTURE,\n",
    "    #         1,\n",
    "    #     ),\n",
    "    # ).astype(\"float32\")\n",
    "    test_data = np.expand_dims(xTest[i], axis=0).astype(\"float32\")\n",
    "    print(test_data.shape)\n",
    "    # Invoke the interpreter\n",
    "    model_quantized_reloaded.set_tensor(model_quantized_input, test_data)\n",
    "    model_quantized_reloaded.invoke()\n",
    "    model_quantized_prediction = model_quantized_reloaded.get_tensor(\n",
    "        model_quantized_output\n",
    "    )\n",
    "\n",
    "    print(\"Digit: {} - Prediction:\\n{}\".format(yTest[i], model_quantized_prediction))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
